{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0567d11-3479-4123-978c-cb98b6676933",
   "metadata": {},
   "source": [
    "### Unit Test Data Creation\n",
    "\n",
    "This notebook outlines the code used to create the unit test data. Some of this is mock data, some real data that has been scrambled, and some is just cuts of real data as time constraints became a concern. The majority of target tables are generated by the functions themselves for brevity, so ensure the target dataframes are generated (if needed) prior to any code changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0818bc-4097-49d5-a6f6-7d2762c9005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import pandas_gbq\n",
    "import numpy as np\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Import from local data files\n",
    "current_path = os.path.abspath('.')\n",
    "sys.path.append(os.path.dirname(current_path))\n",
    "\n",
    "from data_access.data_factory import DataFactory as factory\n",
    "from data_access import prep_pipeline as pp\n",
    "from utils import data as dt\n",
    "from utils import config as cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d72156-98ad-4674-8035-a1db93a2b992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# location in GCP to save tables in\n",
    "# this must end in a full stop \n",
    "table_loc = 'review_ons.'\n",
    "\n",
    "# project ID within GCP to save tables in\n",
    "project_id = 'ons-hotspot-prod'\n",
    "\n",
    "# parameter to overwrite tables in GCP\n",
    "# 'replace' will replace, 'fail' will raise error if the table already exists\n",
    "if_exists = 'replace'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629aa40d-2666-47b0-8576-1a75193fea92",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Two-way fixed effects model\n",
    "\n",
    "The model code is broadly split into two sets of functions - those used in the two-way fixed effect model, and those used in the time tranche model. \n",
    "\n",
    "<b>NB:</b> While some of the data is mocked manually in this notebook, a lot of the dataframes use real data, either sampled, or scrambled. \n",
    "\n",
    "### Static Data\n",
    "\n",
    "Here we create a mock up of the individual tables that are fed into the read_data function when the 'static' table_type parameter is called. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562ccafb-88ec-4b35-881a-97f70528520a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in real data to scramble\n",
    "\n",
    "static_vars = factory.get('static_vars').create_dataframe()\n",
    "\n",
    "static_vars_fake = pd.DataFrame()\n",
    "\n",
    "region_cols = ['LSOA11CD', 'LSOA11NM', 'MSOA11CD', 'MSOA11NM', 'LTLA20CD', 'LTLA20NM',\n",
    "       'UTLA20CD', 'UTLA20NM', 'RGN19CD', 'RGN19NM']\n",
    "\n",
    "static_vars_fake[region_cols] = static_vars[region_cols].sample(n=1000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "static_vars_filt = static_vars[static_vars['LSOA11CD'].str.startswith('E')]\n",
    "\n",
    "# sample data at random so it is not associated with its actual location\n",
    "for col in [col for col in static_vars_filt.columns if col not in region_cols]:\n",
    "    static_vars_fake[col] = static_vars_filt[col].sample(n=1000, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "static_vars_fake.to_gbq(table_loc + 'unit_test_static_vars', project_id = project_id, if_exists = if_exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7666e398-9309-45b3-b92d-79119a6c2ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this dataset is publically available\n",
    "mid_year_lsoa = factory.get('mid_year_lsoa').create_dataframe()\n",
    "\n",
    "lsoa_list = static_vars_fake['LSOA11CD'].unique()\n",
    "mid_year_lsoa = mid_year_lsoa[mid_year_lsoa['LSOA11CD'].isin(lsoa_list)]\n",
    "\n",
    "mid_year_lsoa.to_gbq(table_loc + 'unit_test_mid_year_lsoa', project_id = project_id, if_exists = if_exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848197b3-3324-48fc-8dd3-63a0dc82ef43",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobility_clusters_processed = factory.get('mobility_clusters_processed').create_dataframe()\n",
    "\n",
    "mob = mobility_clusters_processed.sample(n=1000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# replace real LSOAs with 1000 LSOAs we are using in static_vars_fake\n",
    "# as a proxy for shuffling the data\n",
    "mob['LSOA11CD'] = lsoa_list\n",
    "\n",
    "mob.to_gbq(table_loc + 'unit_test_mobility_clusters', project_id = project_id, if_exists = if_exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb50bf9b-d311-4494-a3b4-8eca1a7b75ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_to_work = factory.get('flow_to_work').create_dataframe()\n",
    "\n",
    "flow = flow_to_work.sample(n=1000, random_state=42)\n",
    "\n",
    "# replace real LSOAs with chosen LSOA list as before\n",
    "flow['LSOA11CD'] = lsoa_list\n",
    "\n",
    "flow.to_gbq(table_loc + 'unit_test_flow_to_work', project_id = project_id, if_exists = if_exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d86e3b-5b7e-4572-81b0-0a62dc45eede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is also publically available\n",
    "lsoa_2011 = factory.get('LSOA_2011').create_dataframe() \n",
    "\n",
    "lsoa_2011 = lsoa_2011[lsoa_2011['LSOA11CD'].isin(lsoa_list)]\n",
    "\n",
    "lsoa_2011['geometry'] = lsoa_2011['geometry'].astype(str)\n",
    "\n",
    "lsoa_2011.to_gbq(table_loc + 'unit_test_lsoa_2011', project_id = project_id, if_exists = if_exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca938cff-c1ce-4312-ba76-0d49c4910c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually join the tables as would happen with the read_data function\n",
    "\n",
    "table_list = [mid_year_lsoa,\n",
    "        mob,\n",
    "        flow,\n",
    "        lsoa_2011]\n",
    "\n",
    "df_final = static_vars_fake.copy()\n",
    "\n",
    "for table in table_list:\n",
    "    df_final = df_final.merge(table.copy(), on='LSOA11CD', how='outer', suffixes=['', '_drop'])\n",
    "    \n",
    "drop_cols = [col for col in df_final.columns if col.endswith('_drop')]\n",
    "df_final.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "df_final = df_final[df_final['LSOA11CD'].str.startswith('E')]\n",
    "\n",
    "df_final.to_gbq(table_loc + 'unit_test_static', project_id = project_id, if_exists = if_exists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078c2b93-5a72-4c75-89d1-dbbde95e2d5a",
   "metadata": {},
   "source": [
    "### Dynamic Data\n",
    "\n",
    "Repeating the above steps, but for the dynamic datsets, in this case just cases and mobility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c22972-4700-4a3d-b506-f64ad406a1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cases data\n",
    "cases_df = factory.get('aggregated_tests_lsoa').create_dataframe()\n",
    "\n",
    "lsoas = ['E01001994', 'E01014214', 'E01013400', 'E01002435', 'E01019632',\n",
    "       'E01015272', 'E01030378', 'E01007603', 'E01022044', 'E01007712']\n",
    "\n",
    "dates = pd.date_range(start='2021-01-10', periods=10, freq='w')\n",
    "\n",
    "filt = (cases_df['Date'].isin(dates)) & (cases_df['LSOA11CD'].isin(lsoas))\n",
    "\n",
    "cases_df_sub = cases_df[filt]\n",
    "\n",
    "cases_df_sub.to_gbq(table_loc + 'unit_test_cases', project_id = project_id, if_exists = if_exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5faefb-0f53-4d82-988b-f2f321653e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mobility data\n",
    "deimos_footfall_df = factory.get('lsoa_daily_footfall').create_dataframe()\n",
    "\n",
    "# add extra dates to account for future functions\n",
    "# these operate on the assumption that there will be more mobility data than case data\n",
    "date_list = dates.astype(str).to_list() + ['2021-03-21', '2021-03-28']\n",
    "\n",
    "# adding in a Welsh LSOA to test that this is dealt with appropriately\n",
    "lsoa_list = lsoas + ['W01001957']\n",
    "\n",
    "filt = deimos_footfall_df['Date'].isin(date_list)\n",
    "filt &= deimos_footfall_df['LSOA11CD'].isin(lsoa_list)\n",
    "\n",
    "deimos_mock = deimos_footfall_df[filt]\n",
    "\n",
    "deimos_mock.to_gbq(table_loc + 'unit_test_deimos', project_id = project_id, if_exists = if_exists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b4af1e-ea9d-4aa6-a5c1-b6c6cff31e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating target dataframe for dynamic test of read_data\n",
    "\n",
    "dynamic_mock = cases_df_sub.merge(deimos_mock, on=['LSOA11CD', 'Date'], how='outer', suffixes=['', '_drop'])\n",
    "dynamic_mock.to_gbq(table_loc + 'unit_test_dynamic', project_id = project_id, if_exists = if_exists)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477a9025-5771-4139-bbca-ab7130ecb9e5",
   "metadata": {},
   "source": [
    "### Geo merge function\n",
    "\n",
    "This function simply merges precalculated area data onto LSOA11CD, converting to the required units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1878afd9-a63c-43dd-981d-8a99a0902bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in LSOAs\n",
    "geom_df = factory.get('LSOA_2011').create_dataframe()\n",
    "geom_df = geom_df[['LSOA11CD']]\n",
    "\n",
    "geo_df = pp.geo_merge_precalc(geom_df.copy())\n",
    "\n",
    "geo_df.to_gbq(table_loc + 'unit_test_geo_precalc', project_id = project_id, if_exists = if_exists)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80519d12-5d2b-4669-bfad-c2b226ed4547",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Normalise function\n",
    "\n",
    "Create a dataset which tests the normalise function. This has functionality to normalise data with the sum of each row, or by a separate column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665818b2-132a-403b-95ae-ab4a5aacaf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalise_input = pd.DataFrame({\n",
    "    'col1': range(10),\n",
    "    'col2': range(10, 20),\n",
    "    'col3': range(0,20,2),\n",
    "    'col4': range(0,30,3),\n",
    "    'std': 50\n",
    "})\n",
    "\n",
    "normalise_input.to_gbq(table_loc + 'unit_test_normalise', project_id = project_id, if_exists = if_exists)\n",
    "\n",
    "# the instructions for this normalisation are found in the normalise_dic dictionary\n",
    "# within the config_unit_testing.py file \n",
    "\n",
    "# normalise col1 and col2 by their row-wise sum\n",
    "col_sum = normalise_input['col1'] + normalise_input['col2']\n",
    "normalise_input['col1'] = normalise_input['col1'] / col_sum\n",
    "normalise_input['col2'] = normalise_input['col2'] / col_sum\n",
    "\n",
    "# normalise col3 and col4 by the std column, creating new columns \n",
    "normalise_input['col3_test'] = normalise_input['col3'] / normalise_input['std']\n",
    "normalise_input['col4_test'] = normalise_input['col4'] / normalise_input['std']\n",
    "\n",
    "normalise_input.to_gbq(table_loc + 'unit_test_normalise_result', project_id = project_id, if_exists = if_exists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca33144f-ce2f-44d4-9aea-9b3d69f74b68",
   "metadata": {},
   "source": [
    "### Forward fill function\n",
    "This accounts for data that was cumulative but with incomplete dates - after joining to other datasets, there may be gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440ea625-aaad-4031-a9f1-2a4b9186b1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffill_input = pd.DataFrame({\n",
    "    'col1':[np.nan,  1.,  1.,  2., np.nan,  3.,  4.,  1.,  2.,  3.,  4.,  5.,  6.,\n",
    "        7.,  1., np.nan, np.nan, np.nan,  2.,  4.,  8.,  0.,  0., np.nan, np.nan,  1.,\n",
    "        1., np.nan],\n",
    "    'col2':[0.,  2.,  3., np.nan,  4.,  7.,  7., np.nan,  1.,  1.,  4.,  4., np.nan,\n",
    "       np.nan, np.nan, np.nan,  1.,  1.,  1.,  1.,  1., np.nan, np.nan, np.nan, np.nan, np.nan,\n",
    "       np.nan, np.nan],\n",
    "    'col3':['a']*7 + ['b']*7 + ['c']*7 + ['d']*7,\n",
    "    'date': pd.date_range(start='2020-01-01', periods=7).to_list() * 4\n",
    "})\n",
    "\n",
    "ffill_input.date = ffill_input.date.dt.tz_localize(None)\n",
    "\n",
    "# first output frame where only one column is forward filled\n",
    "ffill_output1 = ffill_input.copy()\n",
    "ffill_output1['col1'] = ffill_output1.groupby('col3')['col1'].fillna(method='ffill')\n",
    "ffill_output1['col1'].fillna(0, inplace=True)\n",
    "\n",
    "# second output frame where two columns are forward filled\n",
    "ffill_output2 = ffill_input.copy()\n",
    "ffill_output2[['col1', 'col2']] = ffill_output2.groupby('col3')['col1', 'col2'].fillna(method='ffill')\n",
    "ffill_output2.fillna(0, inplace=True)\n",
    "\n",
    "ffill_input.to_gbq(table_loc + 'unit_test_ffill_df', project_id = project_id, if_exists = if_exists)\n",
    "ffill_output1.to_gbq(table_loc + 'unit_test_ffill_1', project_id = project_id, if_exists = if_exists)\n",
    "ffill_output2.to_gbq(table_loc + 'unit_test_ffill_2', project_id = project_id, if_exists = if_exists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe92301-1020-43ff-8b1e-5c33a8c31e9e",
   "metadata": {},
   "source": [
    "### Sum features\n",
    "\n",
    "This function sums the columns provided and creates a new column with this sum, dropping the original columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d065f47e-6c65-43f3-a52e-440cf2f4f4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_feat = pd.DataFrame({\n",
    "    'col1': range(5),\n",
    "    'col2': range(5,10),\n",
    "    'col3': range(0,10,2),\n",
    "    'col4': range(18, 3, -3),\n",
    "    'col5': range(5,14, 2)})\n",
    "\n",
    "sum_result = pd.DataFrame()\n",
    "\n",
    "# these parameters are contained in the config unit test file, config_unit_testing.py\n",
    "sum_result['newcol'] = sum_feat[['col1', 'col2']].sum(axis=1)\n",
    "sum_result['newcol2'] = sum_feat[['col3', 'col4', 'col5']].sum(axis=1)\n",
    "\n",
    "sum_feat.to_gbq(table_loc + 'unit_test_sum_features', project_id = project_id, if_exists = if_exists)\n",
    "sum_result.to_gbq(table_loc + 'unit_test_sum_features_result', project_id = project_id, if_exists = if_exists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485cae25-6e6f-46a6-ba1a-cae0bc2d2993",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Timelag function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1004c87d-4c38-4df0-8682-f2d7a0a4934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_df = factory.get('lsoa_dynamic').create_dataframe()\n",
    "dynamic_df_norm = factory.get('dynamic_raw_norm_chosen_geo').create_dataframe()\n",
    "\n",
    "# generate fake data for the input table\n",
    "# take the range of each column and sample randomly from a uniform distribution\n",
    "\n",
    "dyn_cols = ['cases_cumsum', 'full_vacc_cumsum',\n",
    "       'msoa_people',\n",
    "       'worker_footfall_sqkm', 'visitor_footfall_sqkm',\n",
    "       'resident_footfall_sqkm', 'total_footfall_sqkm',\n",
    "       'worker_visitor_footfall_sqkm', 'ALL_PEOPLE', 'Area', #'travel_cluster',\n",
    "       #'RGN19NM', 'UTLA20NM', 'MSOA11NM', 'Country',\n",
    "       'total_vaccinated_first_dose', 'total_vaccinated_second_dose',\n",
    "       'pct_of_people_full_vaccinated', 'cases_per_person',\n",
    "       'pct_infected_all_time', 'COVID_Cases', 'cumsum_divided_area']\n",
    "\n",
    "dynamic_df_mock = pd.DataFrame()\n",
    "\n",
    "for col in dyn_cols:\n",
    "    dyn_min = dynamic_df[col].min()\n",
    "    dyn_max = dynamic_df[col].max()\n",
    "    values = []\n",
    "    for i in range(2000):\n",
    "        values.append(random.uniform(dyn_min, dyn_max))\n",
    "    \n",
    "    dynamic_df_mock[col] = values\n",
    "    \n",
    "dates = pd.date_range(start='2020-01-01', periods = 20)\n",
    "\n",
    "dates_df = pd.DataFrame({'cj':1, 'Date':dates})\n",
    "\n",
    "subset = dynamic_df.sample(n=100, random_state=42)\n",
    "\n",
    "lsoa_dyn = subset['LSOA11CD'].unique()\n",
    "\n",
    "# create a cross-join dataframe to get all combinations of the chosen dates and LSOAs\n",
    "df_cj = pd.DataFrame({'LSOA11CD':lsoa_dyn, \n",
    "                      'travel_cluster': subset['travel_cluster'],\n",
    "                     'cj':1})\n",
    "\n",
    "df_cj = df_cj.merge(dates_df, on='cj')\n",
    "\n",
    "df_cj.drop(columns='cj', inplace=True)\n",
    "\n",
    "dynamic_df_mock[['LSOA11CD', 'travel_cluster', 'Date']] = df_cj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7e9f22-73b5-4b5e-87e3-b97457837d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the same operation for the normalised dynamic dataset\n",
    "dyn_cols = ['COVID_Cases', 'cases_cumsum',\n",
    "       'total_vaccinated_first_dose', 'total_vaccinated_second_dose',\n",
    "       'full_vacc_cumsum',\n",
    "       'msoa_people', 'worker_footfall_sqkm', 'visitor_footfall_sqkm',\n",
    "       'resident_footfall_sqkm', 'total_footfall_sqkm',\n",
    "       'worker_visitor_footfall_sqkm', 'ALL_PEOPLE', 'Area',\n",
    "       #'RGN19NM', 'UTLA20NM', 'MSOA11NM', 'Country', \n",
    "        'Area_chosen_geo',\n",
    "       'Population_chosen_geo', 'total_vaccinated_first_dose_norm_lag_pop',\n",
    "       'total_vaccinated_second_dose_norm_lag_pop',\n",
    "       'full_vacc_cumsum_norm_lag_pop', 'COVID_Cases_norm_lag_pop',\n",
    "       'cases_cumsum_norm_lag_pop', 'COVID_Cases_norm_lag_area',\n",
    "       'cases_cumsum_norm_lag_area', 'worker_footfall_sqkm_norm_lag_area',\n",
    "       'visitor_footfall_sqkm_norm_lag_area',\n",
    "       'resident_footfall_sqkm_norm_lag_area',\n",
    "       'total_footfall_sqkm_norm_lag_area',\n",
    "       'worker_visitor_footfall_sqkm_norm_lag_area',\n",
    "       'commute_inflow_sqkm_norm_lag_area', 'other_inflow_sqkm_norm_lag_area']\n",
    "\n",
    "dynamic_df_norm_mock = pd.DataFrame()\n",
    "\n",
    "for col in dyn_cols:\n",
    "    dyn_min = dynamic_df_norm[col].min()\n",
    "    dyn_max = dynamic_df_norm[col].max()\n",
    "    values = []\n",
    "    for i in range(2000):\n",
    "        values.append(random.uniform(dyn_min, dyn_max))\n",
    "    \n",
    "    dynamic_df_norm_mock[col] = values\n",
    "    \n",
    "dynamic_df_norm_mock[['LSOA11CD', 'travel_cluster', 'Date']] = df_cj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb9c2a4-3301-430c-b2ad-0c8ab33c7650",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# apply the timelag function to get the target dataframe\n",
    "\n",
    "timelag_df = pp.apply_timelag(dynamic_df_mock.copy(), dynamic_df_norm_mock.copy(), save_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8164dfba-9f9d-4941-b66d-162105723cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_df_mock.to_gbq(table_loc + 'unit_test_timelag_dynamic', project_id = project_id, if_exists = if_exists)\n",
    "dynamic_df_norm_mock.to_gbq(table_loc + 'unit_test_timelag_dynamic_norm', project_id = project_id, if_exists = if_exists)\n",
    "timelag_df.to_gbq(table_loc + 'unit_test_timelag_result', project_id = project_id, if_exists = if_exists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851de0d4-68e6-42b7-9ba9-864b62ea4e63",
   "metadata": {},
   "source": [
    "## Time Tranche functions\n",
    "### Static and case data subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b139f4-808a-4aa7-a447-d34517ba671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_df = factory.get('static_features').create_dataframe()\n",
    "\n",
    "# select same set of LSOAs that are in the unit_test_cases table\n",
    "static_df_sub = static_df[static_df['LSOA11CD'].isin(lsoas)]\n",
    "\n",
    "# add Area column - this was removed from the source dataframe as is no longer required in the script\n",
    "# so needs to be added back here manually\n",
    "# this also needs to be done for the LSOA population column, ALL_PEOPLE\n",
    "area_list = [x/10 for x in range(1, 200, 5)]\n",
    "\n",
    "# set seed for reproducibility\n",
    "random.seed(42)\n",
    "static_df_sub['Area'] = random.sample(area_list, 10)\n",
    "static_df_sub['ALL_PEOPLE'] = random.sample(range(1400,2200), 10)       \n",
    "\n",
    "static_df_sub.to_gbq(table_loc + 'unit_test_static_for_cases', project_id = project_id, if_exists = if_exists)\n",
    "\n",
    "cases_static = pp.join_cases_to_static_data(static_df_sub, 'unit_test_cases')\n",
    "\n",
    "cases_static.to_gbq(table_loc + 'unit_test_cases_static',project_id = project_id, if_exists = if_exists)\n",
    "\n",
    "cases_static_week = pp.derive_week_number(cases_static)\n",
    "cases_static_week.to_gbq(table_loc + 'unit_test_cases_static_week', project_id = project_id, if_exists = if_exists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ca7369-7688-4b39-ba27-c2b09a478c3d",
   "metadata": {},
   "source": [
    "### Vaccination data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd256a5b-46dc-4906-b629-6f74bcd2dd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vax_df = factory.get('lsoa_vaccinations').create_dataframe()\n",
    "\n",
    "cases_static_week['Date'] = pd.to_datetime(cases_static_week['Date']).dt.date.astype(str)\n",
    "\n",
    "filt = vax_df['LSOA11CD'].isin(cases_static_week['LSOA11CD'].unique())\n",
    "filt &= vax_df['Date'].isin(list(cases_static_week['Date'].unique()) + ['2021-03-21', '2021-03-28'])\n",
    "\n",
    "vax_df_sub = vax_df[filt]    \n",
    "\n",
    "vax_df_sub.sort_values(by=['LSOA11CD', 'Date'], inplace=True)\n",
    "vax_df_sub.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# delete first row so fillna in join_vax_data function will have something to work on\n",
    "vax_df_sub = vax_df_sub.iloc[1:,:]\n",
    "vax_df_sub.reset_index(drop=True, inplace=True)\n",
    "\n",
    "cases_static_week['Date'] = pd.to_datetime(cases_static_week['Date']).dt.date.astype(str)\n",
    "\n",
    "vax_processed_df, cases_all_weeks_df = pp.join_vax_data(cases_static_week, vax_df_sub)\n",
    "\n",
    "vax_df_sub.to_gbq(table_loc + 'unit_test_vaccinations', project_id = project_id, if_exists = if_exists)\n",
    "vax_processed_df.to_gbq(table_loc + 'unit_test_vaccinations_processed', project_id = project_id, if_exists = if_exists)\n",
    "cases_all_weeks_df.to_gbq(table_loc + 'unit_test_cases_static_week_vax', project_id = project_id, if_exists = if_exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee54d6b6-43b4-46b6-be22-7ea2efc92dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure Date columns can be merged\n",
    "deimos_mock['Date'] = pd.to_datetime(deimos_mock['Date']).dt.date.astype(str)\n",
    "\n",
    "joined = pp.join_tranches_mobility_data(cases_all_weeks_df.copy(), deimos_mock.copy())\n",
    "joined.to_gbq(table_loc + 'unit_test_deimos_cases_vax', project_id = project_id, if_exists = if_exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772890e4-5a0d-4426-8788-6cc98e233959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first target for the convert_unit function, changing the original column\n",
    "# NB this function will also change the original dataframe, so be sure to copy this if you wish to retain the original\n",
    "convert_units_df = pp.convert_units(joined.copy(), \n",
    "                                    'meat_and_fish_processing',\n",
    "                                    0.1)\n",
    "convert_units_df = pp.sort_cols(convert_units_df, ['LSOA11CD', 'Date'])\n",
    "\n",
    "convert_units_df.to_gbq(table_loc + 'unit_test_convert_unit', project_id = project_id, if_exists = if_exists)\n",
    "\n",
    "# second target, creating a new column\n",
    "convert_units_df_alt = pp.convert_units(joined.copy(), \n",
    "                                    'meat_and_fish_processing',\n",
    "                                    0.1,\n",
    "                                    new_colname='meat_and_fish_processing_alt')\n",
    "\n",
    "convert_units_df_alt = pp.sort_cols(convert_units_df_alt, ['LSOA11CD', 'Date'])\n",
    "convert_units_df_alt.to_gbq(table_loc + 'unit_test_convert_unit_alt', project_id = project_id, if_exists = if_exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26468b36-385c-4dc4-bc1f-145080105931",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pp.create_test_data(convert_units_df, static_df_sub, deimos_mock, vax_processed_df)\n",
    "\n",
    "test_data.to_gbq(table_loc + 'unit_test_tranche_test', project_id = project_id, if_exists = if_exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f2c787-2bfd-4a1e-8b57-e67c046d3a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_tranche_df = pp.create_time_tranches(convert_units_df,\n",
    "                                         tranche_dates=cf.tranche_dates,\n",
    "                                         tranche_description = cf.tranche_description)\n",
    "\n",
    "time_tranche_df.to_gbq(table_loc + 'unit_test_time_tranche', project_id = project_id, if_exists = if_exists)\n",
    "\n",
    "tranche_order = pp.derive_tranche_order(time_tranche_df, \n",
    "                                        tranche_description=cf.tranche_description)\n",
    "\n",
    "tranche_order.to_gbq(table_loc + 'unit_test_tranche_order', project_id = project_id, if_exists = if_exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbff277b-3b26-4823-aa7c-55a2e3e1675c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-10.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-10:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
